{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z']\n"
     ]
    }
   ],
   "source": [
    "path = r'C:\\Users\\ishit\\Downloads\\Image\\Image'\n",
    "files = os.listdir(path)\n",
    "files.sort()\n",
    "print(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_array = []\n",
    "label_array = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26/26 [00:11<00:00,  2.32it/s]\n"
     ]
    }
   ],
   "source": [
    "for i in tqdm(range(len(files))):\n",
    "\tsub_file=os.listdir(path+\"/\"+files[i])\n",
    "\n",
    "\tfor j in range(len(sub_file)):\n",
    "\n",
    "\t\tfile_path=path+\"/\"+files[i]+\"/\"+sub_file[j]\n",
    "\n",
    "\t\timage=cv2.imread(file_path)\n",
    "\n",
    "\t\timage=cv2.resize(image,(96,96))\n",
    "\n",
    "\t\timage=cv2.cvtColor(image,cv2.COLOR_BGR2RGB)\n",
    "\n",
    "\n",
    "\t\timage_array.append(image)\n",
    "\n",
    "\t\tlabel_array.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_array=np.array(image_array)\n",
    "label_array=np.array(label_array,dtype=\"float\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "# output\t\t\t\t\t\t\t\t\t   train image   label      spliting size\n",
    "X_train,X_test,Y_train,Y_test=train_test_split(image_array,label_array,test_size=0.15)\n",
    "\n",
    "del image_array,label_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import layers,callbacks,utils,applications,optimizers\n",
    "from keras.models import Sequential, Model, load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/keras-applications/efficientnetb0_notop.h5\n",
      "16705208/16705208 [==============================] - 2s 0us/step\n"
     ]
    }
   ],
   "source": [
    "pretrained_model=tf.keras.applications.EfficientNetB0(input_shape=(96,96,3),include_top=False)\n",
    "model.add(pretrained_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(layers.GlobalAveragePooling2D())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(layers.Dropout(0.3))\n",
    "model.add(layers.Dense(1))\n",
    "model.build(input_shape=(None,96,96,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " efficientnetb0 (Functional)  (None, 3, 3, 1280)       4049571   \n",
      "                                                                 \n",
      " global_average_pooling2d (G  (None, 1280)             0         \n",
      " lobalAveragePooling2D)                                          \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 1280)              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1)                 1281      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 4,050,852\n",
      "Trainable params: 4,008,829\n",
      "Non-trainable params: 42,023\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=\"adam\",loss=\"mae\",metrics=[\"mae\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "ckp_path=\"trained_model/model\"\n",
    "model_checkpoint=tf.keras.callbacks.ModelCheckpoint(\n",
    "\t\t\t\t\t\t\t\t\tfilepath=ckp_path,\n",
    "\t\t\t\t\t\t\t\t\tmonitor=\"val_mae\",\n",
    "\t\t\t\t\t\t\t\t\tmode=\"auto\",\n",
    "\t\t\t\t\t\t\t\t\tsave_best_only=True,\n",
    "\t\t\t\t\t\t\t\t\tsave_weights_only=True\n",
    "\t\t\t\t\t\t\t\t\t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduce_lr=tf.keras.callbacks.ReduceLROnPlateau(\n",
    "\t\t\t\t\t\t\t\t\tfactor=0.9,\n",
    "\t\t\t\t\t\t\t\t\tmonitor=\"val_mae\",\n",
    "\t\t\t\t\t\t\t\t\tmode=\"auto\",\n",
    "\t\t\t\t\t\t\t\t\tcooldown=0,\n",
    "\t\t\t\t\t\t\t\t\tpatience=5,\n",
    "\t\t\t\t\t\t\t\t\tverbose=1,\n",
    "\t\t\t\t\t\t\t\t\tmin_lr=1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "Epochs=100\n",
    "Batch_Size=32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "82/82 [==============================] - 201s 2s/step - loss: 4.0159 - mae: 4.0159 - val_loss: 16.3841 - val_mae: 16.3841 - lr: 0.0010\n",
      "Epoch 2/100\n",
      "82/82 [==============================] - 164s 2s/step - loss: 1.3112 - mae: 1.3112 - val_loss: 6.5929 - val_mae: 6.5929 - lr: 0.0010\n",
      "Epoch 3/100\n",
      "82/82 [==============================] - 162s 2s/step - loss: 1.0247 - mae: 1.0247 - val_loss: 1.6710 - val_mae: 1.6710 - lr: 0.0010\n",
      "Epoch 4/100\n",
      "82/82 [==============================] - 162s 2s/step - loss: 0.7540 - mae: 0.7540 - val_loss: 1.1990 - val_mae: 1.1990 - lr: 0.0010\n",
      "Epoch 5/100\n",
      "82/82 [==============================] - 159s 2s/step - loss: 0.7106 - mae: 0.7106 - val_loss: 1.0221 - val_mae: 1.0221 - lr: 0.0010\n",
      "Epoch 6/100\n",
      "82/82 [==============================] - 158s 2s/step - loss: 0.7200 - mae: 0.7200 - val_loss: 2.0932 - val_mae: 2.0932 - lr: 0.0010\n",
      "Epoch 7/100\n",
      "82/82 [==============================] - 159s 2s/step - loss: 0.6734 - mae: 0.6734 - val_loss: 1.1375 - val_mae: 1.1375 - lr: 0.0010\n",
      "Epoch 8/100\n",
      "82/82 [==============================] - 159s 2s/step - loss: 0.7078 - mae: 0.7078 - val_loss: 1.3522 - val_mae: 1.3522 - lr: 0.0010\n",
      "Epoch 9/100\n",
      "82/82 [==============================] - 159s 2s/step - loss: 0.5198 - mae: 0.5198 - val_loss: 0.4856 - val_mae: 0.4856 - lr: 0.0010\n",
      "Epoch 10/100\n",
      "82/82 [==============================] - 161s 2s/step - loss: 0.4857 - mae: 0.4857 - val_loss: 1.6918 - val_mae: 1.6918 - lr: 0.0010\n",
      "Epoch 11/100\n",
      "82/82 [==============================] - 167s 2s/step - loss: 0.5406 - mae: 0.5406 - val_loss: 0.6267 - val_mae: 0.6267 - lr: 0.0010\n",
      "Epoch 12/100\n",
      "82/82 [==============================] - 169s 2s/step - loss: 0.5141 - mae: 0.5141 - val_loss: 0.5162 - val_mae: 0.5162 - lr: 0.0010\n",
      "Epoch 13/100\n",
      "82/82 [==============================] - 164s 2s/step - loss: 0.5490 - mae: 0.5490 - val_loss: 1.2460 - val_mae: 1.2460 - lr: 0.0010\n",
      "Epoch 14/100\n",
      "82/82 [==============================] - ETA: 0s - loss: 0.4891 - mae: 0.4891\n",
      "Epoch 14: ReduceLROnPlateau reducing learning rate to 0.0009000000427477062.\n",
      "82/82 [==============================] - 162s 2s/step - loss: 0.4891 - mae: 0.4891 - val_loss: 0.9663 - val_mae: 0.9663 - lr: 0.0010\n",
      "Epoch 15/100\n",
      "82/82 [==============================] - 161s 2s/step - loss: 0.4630 - mae: 0.4630 - val_loss: 0.3034 - val_mae: 0.3034 - lr: 9.0000e-04\n",
      "Epoch 16/100\n",
      "82/82 [==============================] - 161s 2s/step - loss: 0.4392 - mae: 0.4392 - val_loss: 0.9343 - val_mae: 0.9343 - lr: 9.0000e-04\n",
      "Epoch 17/100\n",
      "82/82 [==============================] - 160s 2s/step - loss: 0.4234 - mae: 0.4234 - val_loss: 0.3582 - val_mae: 0.3582 - lr: 9.0000e-04\n",
      "Epoch 18/100\n",
      "82/82 [==============================] - 161s 2s/step - loss: 0.4093 - mae: 0.4093 - val_loss: 0.1941 - val_mae: 0.1941 - lr: 9.0000e-04\n",
      "Epoch 19/100\n",
      "82/82 [==============================] - 160s 2s/step - loss: 0.4545 - mae: 0.4545 - val_loss: 0.3696 - val_mae: 0.3696 - lr: 9.0000e-04\n",
      "Epoch 20/100\n",
      "82/82 [==============================] - 160s 2s/step - loss: 0.4484 - mae: 0.4484 - val_loss: 0.2104 - val_mae: 0.2104 - lr: 9.0000e-04\n",
      "Epoch 21/100\n",
      "82/82 [==============================] - 161s 2s/step - loss: 0.4211 - mae: 0.4211 - val_loss: 0.3308 - val_mae: 0.3308 - lr: 9.0000e-04\n",
      "Epoch 22/100\n",
      "82/82 [==============================] - 160s 2s/step - loss: 0.3855 - mae: 0.3855 - val_loss: 0.6212 - val_mae: 0.6212 - lr: 9.0000e-04\n",
      "Epoch 23/100\n",
      "82/82 [==============================] - ETA: 0s - loss: 0.4142 - mae: 0.4142\n",
      "Epoch 23: ReduceLROnPlateau reducing learning rate to 0.0008100000384729356.\n",
      "82/82 [==============================] - 160s 2s/step - loss: 0.4142 - mae: 0.4142 - val_loss: 0.3736 - val_mae: 0.3736 - lr: 9.0000e-04\n",
      "Epoch 24/100\n",
      "82/82 [==============================] - 160s 2s/step - loss: 0.3787 - mae: 0.3787 - val_loss: 0.4435 - val_mae: 0.4435 - lr: 8.1000e-04\n",
      "Epoch 25/100\n",
      "82/82 [==============================] - 161s 2s/step - loss: 0.4094 - mae: 0.4094 - val_loss: 0.2361 - val_mae: 0.2361 - lr: 8.1000e-04\n",
      "Epoch 26/100\n",
      "82/82 [==============================] - 160s 2s/step - loss: 0.4003 - mae: 0.4003 - val_loss: 0.7369 - val_mae: 0.7369 - lr: 8.1000e-04\n",
      "Epoch 27/100\n",
      "82/82 [==============================] - 160s 2s/step - loss: 0.3799 - mae: 0.3799 - val_loss: 0.3775 - val_mae: 0.3775 - lr: 8.1000e-04\n",
      "Epoch 28/100\n",
      "82/82 [==============================] - 160s 2s/step - loss: 0.3825 - mae: 0.3825 - val_loss: 0.1645 - val_mae: 0.1645 - lr: 8.1000e-04\n",
      "Epoch 29/100\n",
      "82/82 [==============================] - 160s 2s/step - loss: 0.3702 - mae: 0.3702 - val_loss: 0.1936 - val_mae: 0.1936 - lr: 8.1000e-04\n",
      "Epoch 30/100\n",
      "82/82 [==============================] - 163s 2s/step - loss: 0.3700 - mae: 0.3700 - val_loss: 0.6945 - val_mae: 0.6945 - lr: 8.1000e-04\n",
      "Epoch 31/100\n",
      "82/82 [==============================] - 161s 2s/step - loss: 0.4042 - mae: 0.4042 - val_loss: 0.1422 - val_mae: 0.1422 - lr: 8.1000e-04\n",
      "Epoch 32/100\n",
      "82/82 [==============================] - 160s 2s/step - loss: 0.3983 - mae: 0.3983 - val_loss: 0.3758 - val_mae: 0.3758 - lr: 8.1000e-04\n",
      "Epoch 33/100\n",
      "82/82 [==============================] - 161s 2s/step - loss: 0.3530 - mae: 0.3530 - val_loss: 0.1688 - val_mae: 0.1688 - lr: 8.1000e-04\n",
      "Epoch 34/100\n",
      "82/82 [==============================] - 159s 2s/step - loss: 0.3433 - mae: 0.3433 - val_loss: 0.1863 - val_mae: 0.1863 - lr: 8.1000e-04\n",
      "Epoch 35/100\n",
      "82/82 [==============================] - 159s 2s/step - loss: 0.3478 - mae: 0.3478 - val_loss: 0.7000 - val_mae: 0.7000 - lr: 8.1000e-04\n",
      "Epoch 36/100\n",
      "82/82 [==============================] - ETA: 0s - loss: 0.3476 - mae: 0.3476\n",
      "Epoch 36: ReduceLROnPlateau reducing learning rate to 0.0007290000503417104.\n",
      "82/82 [==============================] - 159s 2s/step - loss: 0.3476 - mae: 0.3476 - val_loss: 0.3303 - val_mae: 0.3303 - lr: 8.1000e-04\n",
      "Epoch 37/100\n",
      "82/82 [==============================] - 159s 2s/step - loss: 0.3315 - mae: 0.3315 - val_loss: 0.2181 - val_mae: 0.2181 - lr: 7.2900e-04\n",
      "Epoch 38/100\n",
      "82/82 [==============================] - 161s 2s/step - loss: 0.3454 - mae: 0.3454 - val_loss: 0.1666 - val_mae: 0.1666 - lr: 7.2900e-04\n",
      "Epoch 39/100\n",
      "82/82 [==============================] - 159s 2s/step - loss: 0.3472 - mae: 0.3472 - val_loss: 0.5015 - val_mae: 0.5015 - lr: 7.2900e-04\n",
      "Epoch 40/100\n",
      "82/82 [==============================] - 160s 2s/step - loss: 0.3685 - mae: 0.3685 - val_loss: 0.3680 - val_mae: 0.3680 - lr: 7.2900e-04\n",
      "Epoch 41/100\n",
      "82/82 [==============================] - ETA: 0s - loss: 0.3803 - mae: 0.3803\n",
      "Epoch 41: ReduceLROnPlateau reducing learning rate to 0.0006561000715009868.\n",
      "82/82 [==============================] - 159s 2s/step - loss: 0.3803 - mae: 0.3803 - val_loss: 0.2136 - val_mae: 0.2136 - lr: 7.2900e-04\n",
      "Epoch 42/100\n",
      "82/82 [==============================] - 160s 2s/step - loss: 0.3448 - mae: 0.3448 - val_loss: 0.3762 - val_mae: 0.3762 - lr: 6.5610e-04\n",
      "Epoch 43/100\n",
      "82/82 [==============================] - 159s 2s/step - loss: 0.3471 - mae: 0.3471 - val_loss: 0.3818 - val_mae: 0.3818 - lr: 6.5610e-04\n",
      "Epoch 44/100\n",
      "82/82 [==============================] - 159s 2s/step - loss: 0.3226 - mae: 0.3226 - val_loss: 0.2705 - val_mae: 0.2705 - lr: 6.5610e-04\n",
      "Epoch 45/100\n",
      "82/82 [==============================] - 157s 2s/step - loss: 0.3275 - mae: 0.3275 - val_loss: 0.5866 - val_mae: 0.5866 - lr: 6.5610e-04\n",
      "Epoch 46/100\n",
      "82/82 [==============================] - ETA: 0s - loss: 0.3352 - mae: 0.3352\n",
      "Epoch 46: ReduceLROnPlateau reducing learning rate to 0.0005904900433961303.\n",
      "82/82 [==============================] - 157s 2s/step - loss: 0.3352 - mae: 0.3352 - val_loss: 0.3160 - val_mae: 0.3160 - lr: 6.5610e-04\n",
      "Epoch 47/100\n",
      "82/82 [==============================] - 157s 2s/step - loss: 0.3333 - mae: 0.3333 - val_loss: 0.1118 - val_mae: 0.1118 - lr: 5.9049e-04\n",
      "Epoch 48/100\n",
      "82/82 [==============================] - 157s 2s/step - loss: 0.3012 - mae: 0.3012 - val_loss: 0.2269 - val_mae: 0.2269 - lr: 5.9049e-04\n",
      "Epoch 49/100\n",
      "82/82 [==============================] - 157s 2s/step - loss: 0.3245 - mae: 0.3245 - val_loss: 0.4199 - val_mae: 0.4199 - lr: 5.9049e-04\n",
      "Epoch 50/100\n",
      "82/82 [==============================] - 157s 2s/step - loss: 0.3275 - mae: 0.3275 - val_loss: 0.2444 - val_mae: 0.2444 - lr: 5.9049e-04\n",
      "Epoch 51/100\n",
      "82/82 [==============================] - 160s 2s/step - loss: 0.3493 - mae: 0.3493 - val_loss: 0.0942 - val_mae: 0.0942 - lr: 5.9049e-04\n",
      "Epoch 52/100\n",
      "82/82 [==============================] - 195s 2s/step - loss: 0.3057 - mae: 0.3057 - val_loss: 0.1742 - val_mae: 0.1742 - lr: 5.9049e-04\n",
      "Epoch 53/100\n",
      "82/82 [==============================] - 187s 2s/step - loss: 0.3201 - mae: 0.3201 - val_loss: 0.1121 - val_mae: 0.1121 - lr: 5.9049e-04\n",
      "Epoch 54/100\n",
      "82/82 [==============================] - 177s 2s/step - loss: 0.3145 - mae: 0.3145 - val_loss: 0.1754 - val_mae: 0.1754 - lr: 5.9049e-04\n",
      "Epoch 55/100\n",
      "82/82 [==============================] - 183s 2s/step - loss: 0.2987 - mae: 0.2987 - val_loss: 0.3665 - val_mae: 0.3665 - lr: 5.9049e-04\n",
      "Epoch 56/100\n",
      "82/82 [==============================] - ETA: 0s - loss: 0.3191 - mae: 0.3191\n",
      "Epoch 56: ReduceLROnPlateau reducing learning rate to 0.0005314410547725857.\n",
      "82/82 [==============================] - 160s 2s/step - loss: 0.3191 - mae: 0.3191 - val_loss: 0.1565 - val_mae: 0.1565 - lr: 5.9049e-04\n",
      "Epoch 57/100\n",
      "82/82 [==============================] - 159s 2s/step - loss: 0.3082 - mae: 0.3082 - val_loss: 0.1332 - val_mae: 0.1332 - lr: 5.3144e-04\n",
      "Epoch 58/100\n",
      "82/82 [==============================] - 159s 2s/step - loss: 0.3181 - mae: 0.3181 - val_loss: 0.1603 - val_mae: 0.1603 - lr: 5.3144e-04\n",
      "Epoch 59/100\n",
      "82/82 [==============================] - 158s 2s/step - loss: 0.3326 - mae: 0.3326 - val_loss: 0.2706 - val_mae: 0.2706 - lr: 5.3144e-04\n",
      "Epoch 60/100\n",
      "82/82 [==============================] - 158s 2s/step - loss: 0.3151 - mae: 0.3151 - val_loss: 0.1284 - val_mae: 0.1284 - lr: 5.3144e-04\n",
      "Epoch 61/100\n",
      "82/82 [==============================] - ETA: 0s - loss: 0.3159 - mae: 0.3159\n",
      "Epoch 61: ReduceLROnPlateau reducing learning rate to 0.00047829695977270604.\n",
      "82/82 [==============================] - 183s 2s/step - loss: 0.3159 - mae: 0.3159 - val_loss: 0.3979 - val_mae: 0.3979 - lr: 5.3144e-04\n",
      "Epoch 62/100\n",
      "82/82 [==============================] - 181s 2s/step - loss: 0.2920 - mae: 0.2920 - val_loss: 0.3780 - val_mae: 0.3780 - lr: 4.7830e-04\n",
      "Epoch 63/100\n",
      "82/82 [==============================] - 166s 2s/step - loss: 0.3053 - mae: 0.3053 - val_loss: 0.1580 - val_mae: 0.1580 - lr: 4.7830e-04\n",
      "Epoch 64/100\n",
      "82/82 [==============================] - 159s 2s/step - loss: 0.2940 - mae: 0.2940 - val_loss: 0.1243 - val_mae: 0.1243 - lr: 4.7830e-04\n",
      "Epoch 65/100\n",
      "82/82 [==============================] - 160s 2s/step - loss: 0.3033 - mae: 0.3033 - val_loss: 0.3350 - val_mae: 0.3350 - lr: 4.7830e-04\n",
      "Epoch 66/100\n",
      "82/82 [==============================] - ETA: 0s - loss: 0.2957 - mae: 0.2957\n",
      "Epoch 66: ReduceLROnPlateau reducing learning rate to 0.0004304672533180565.\n",
      "82/82 [==============================] - 158s 2s/step - loss: 0.2957 - mae: 0.2957 - val_loss: 0.1657 - val_mae: 0.1657 - lr: 4.7830e-04\n",
      "Epoch 67/100\n",
      "82/82 [==============================] - 158s 2s/step - loss: 0.2861 - mae: 0.2861 - val_loss: 0.1219 - val_mae: 0.1219 - lr: 4.3047e-04\n",
      "Epoch 68/100\n",
      "82/82 [==============================] - 158s 2s/step - loss: 0.2847 - mae: 0.2847 - val_loss: 0.1289 - val_mae: 0.1289 - lr: 4.3047e-04\n",
      "Epoch 69/100\n",
      "82/82 [==============================] - 158s 2s/step - loss: 0.2812 - mae: 0.2812 - val_loss: 0.1393 - val_mae: 0.1393 - lr: 4.3047e-04\n",
      "Epoch 70/100\n",
      "82/82 [==============================] - 158s 2s/step - loss: 0.2771 - mae: 0.2771 - val_loss: 0.2551 - val_mae: 0.2551 - lr: 4.3047e-04\n",
      "Epoch 71/100\n",
      "82/82 [==============================] - ETA: 0s - loss: 0.2855 - mae: 0.2855\n",
      "Epoch 71: ReduceLROnPlateau reducing learning rate to 0.00038742052274756136.\n",
      "82/82 [==============================] - 158s 2s/step - loss: 0.2855 - mae: 0.2855 - val_loss: 0.1038 - val_mae: 0.1038 - lr: 4.3047e-04\n",
      "Epoch 72/100\n",
      "82/82 [==============================] - 158s 2s/step - loss: 0.2696 - mae: 0.2696 - val_loss: 0.1387 - val_mae: 0.1387 - lr: 3.8742e-04\n",
      "Epoch 73/100\n",
      "82/82 [==============================] - 169s 2s/step - loss: 0.2825 - mae: 0.2825 - val_loss: 0.1160 - val_mae: 0.1160 - lr: 3.8742e-04\n",
      "Epoch 74/100\n",
      "82/82 [==============================] - 188s 2s/step - loss: 0.2853 - mae: 0.2853 - val_loss: 0.0685 - val_mae: 0.0685 - lr: 3.8742e-04\n",
      "Epoch 75/100\n",
      "82/82 [==============================] - 195s 2s/step - loss: 0.2988 - mae: 0.2988 - val_loss: 0.2878 - val_mae: 0.2878 - lr: 3.8742e-04\n",
      "Epoch 76/100\n",
      "82/82 [==============================] - 158s 2s/step - loss: 0.2851 - mae: 0.2851 - val_loss: 0.1884 - val_mae: 0.1884 - lr: 3.8742e-04\n",
      "Epoch 77/100\n",
      "82/82 [==============================] - 159s 2s/step - loss: 0.2826 - mae: 0.2826 - val_loss: 0.0918 - val_mae: 0.0918 - lr: 3.8742e-04\n",
      "Epoch 78/100\n",
      "82/82 [==============================] - 158s 2s/step - loss: 0.2772 - mae: 0.2772 - val_loss: 0.0832 - val_mae: 0.0832 - lr: 3.8742e-04\n",
      "Epoch 79/100\n",
      "82/82 [==============================] - ETA: 0s - loss: 0.2722 - mae: 0.2722\n",
      "Epoch 79: ReduceLROnPlateau reducing learning rate to 0.0003486784757114947.\n",
      "82/82 [==============================] - 158s 2s/step - loss: 0.2722 - mae: 0.2722 - val_loss: 0.1041 - val_mae: 0.1041 - lr: 3.8742e-04\n",
      "Epoch 80/100\n",
      "82/82 [==============================] - 158s 2s/step - loss: 0.2944 - mae: 0.2944 - val_loss: 0.3220 - val_mae: 0.3220 - lr: 3.4868e-04\n",
      "Epoch 81/100\n",
      "82/82 [==============================] - 158s 2s/step - loss: 0.2609 - mae: 0.2609 - val_loss: 0.1601 - val_mae: 0.1601 - lr: 3.4868e-04\n",
      "Epoch 82/100\n",
      "82/82 [==============================] - 159s 2s/step - loss: 0.2684 - mae: 0.2684 - val_loss: 0.0620 - val_mae: 0.0620 - lr: 3.4868e-04\n",
      "Epoch 83/100\n",
      "82/82 [==============================] - 158s 2s/step - loss: 0.2697 - mae: 0.2697 - val_loss: 0.1704 - val_mae: 0.1704 - lr: 3.4868e-04\n",
      "Epoch 84/100\n",
      "82/82 [==============================] - 158s 2s/step - loss: 0.2746 - mae: 0.2746 - val_loss: 0.0676 - val_mae: 0.0676 - lr: 3.4868e-04\n",
      "Epoch 85/100\n",
      "82/82 [==============================] - 158s 2s/step - loss: 0.2843 - mae: 0.2843 - val_loss: 0.1217 - val_mae: 0.1217 - lr: 3.4868e-04\n",
      "Epoch 86/100\n",
      "82/82 [==============================] - 158s 2s/step - loss: 0.2610 - mae: 0.2610 - val_loss: 0.0954 - val_mae: 0.0954 - lr: 3.4868e-04\n",
      "Epoch 87/100\n",
      "82/82 [==============================] - ETA: 0s - loss: 0.2823 - mae: 0.2823\n",
      "Epoch 87: ReduceLROnPlateau reducing learning rate to 0.00031381062290165574.\n",
      "82/82 [==============================] - 158s 2s/step - loss: 0.2823 - mae: 0.2823 - val_loss: 0.3857 - val_mae: 0.3857 - lr: 3.4868e-04\n",
      "Epoch 88/100\n",
      "82/82 [==============================] - 163s 2s/step - loss: 0.2757 - mae: 0.2757 - val_loss: 0.0718 - val_mae: 0.0718 - lr: 3.1381e-04\n",
      "Epoch 89/100\n",
      "82/82 [==============================] - 158s 2s/step - loss: 0.2875 - mae: 0.2875 - val_loss: 0.2049 - val_mae: 0.2049 - lr: 3.1381e-04\n",
      "Epoch 90/100\n",
      "82/82 [==============================] - 158s 2s/step - loss: 0.2669 - mae: 0.2669 - val_loss: 0.0958 - val_mae: 0.0958 - lr: 3.1381e-04\n",
      "Epoch 91/100\n",
      "82/82 [==============================] - 158s 2s/step - loss: 0.2669 - mae: 0.2669 - val_loss: 0.2678 - val_mae: 0.2678 - lr: 3.1381e-04\n",
      "Epoch 92/100\n",
      "82/82 [==============================] - ETA: 0s - loss: 0.2712 - mae: 0.2712\n",
      "Epoch 92: ReduceLROnPlateau reducing learning rate to 0.0002824295632308349.\n",
      "82/82 [==============================] - 158s 2s/step - loss: 0.2712 - mae: 0.2712 - val_loss: 0.1365 - val_mae: 0.1365 - lr: 3.1381e-04\n",
      "Epoch 93/100\n",
      "82/82 [==============================] - 158s 2s/step - loss: 0.2619 - mae: 0.2619 - val_loss: 0.0940 - val_mae: 0.0940 - lr: 2.8243e-04\n",
      "Epoch 94/100\n",
      "82/82 [==============================] - 167s 2s/step - loss: 0.2681 - mae: 0.2681 - val_loss: 0.2849 - val_mae: 0.2849 - lr: 2.8243e-04\n",
      "Epoch 95/100\n",
      "82/82 [==============================] - 158s 2s/step - loss: 0.2868 - mae: 0.2868 - val_loss: 0.0947 - val_mae: 0.0947 - lr: 2.8243e-04\n",
      "Epoch 96/100\n",
      "82/82 [==============================] - 159s 2s/step - loss: 0.2735 - mae: 0.2735 - val_loss: 0.1127 - val_mae: 0.1127 - lr: 2.8243e-04\n",
      "Epoch 97/100\n",
      "82/82 [==============================] - ETA: 0s - loss: 0.2860 - mae: 0.2860\n",
      "Epoch 97: ReduceLROnPlateau reducing learning rate to 0.00025418660952709616.\n",
      "82/82 [==============================] - 159s 2s/step - loss: 0.2860 - mae: 0.2860 - val_loss: 0.0708 - val_mae: 0.0708 - lr: 2.8243e-04\n",
      "Epoch 98/100\n",
      "82/82 [==============================] - 158s 2s/step - loss: 0.2696 - mae: 0.2696 - val_loss: 0.1949 - val_mae: 0.1949 - lr: 2.5419e-04\n",
      "Epoch 99/100\n",
      "82/82 [==============================] - 158s 2s/step - loss: 0.2840 - mae: 0.2840 - val_loss: 0.0811 - val_mae: 0.0811 - lr: 2.5419e-04\n",
      "Epoch 100/100\n",
      "82/82 [==============================] - 158s 2s/step - loss: 0.2523 - mae: 0.2523 - val_loss: 0.0687 - val_mae: 0.0687 - lr: 2.5419e-04\n"
     ]
    }
   ],
   "source": [
    "history=model.fit(\n",
    "\t\t\t\tX_train,\n",
    "\t\t\t\tY_train,\n",
    "\t\t\t\tvalidation_data=(X_test,Y_test),\n",
    "\t\t\t\tbatch_size=Batch_Size,\n",
    "\t\t\t\tepochs=Epochs,\n",
    "\t\t\t\tcallbacks=[model_checkpoint,reduce_lr]\n",
    "\t\t\t\t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.checkpoint.checkpoint.CheckpointLoadStatus at 0x2247b011e50>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_weights(ckp_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _update_step_xla, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 82). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\ishit\\AppData\\Local\\Temp\\tmpo02koiss\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\ishit\\AppData\\Local\\Temp\\tmpo02koiss\\assets\n"
     ]
    }
   ],
   "source": [
    "converter=tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "tflite_model=converter.convert()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"model.tflite\",\"wb\") as f:\n",
    "\tf.write(tflite_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15/15 [==============================] - 11s 507ms/step\n"
     ]
    }
   ],
   "source": [
    "prediction_val=model.predict(X_test,batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.06613109]\n",
      " [15.10586   ]\n",
      " [ 9.135483  ]\n",
      " [24.054834  ]\n",
      " [ 4.073693  ]\n",
      " [18.012497  ]\n",
      " [ 6.973255  ]\n",
      " [ 4.070263  ]\n",
      " [10.099316  ]\n",
      " [15.113444  ]]\n"
     ]
    }
   ],
   "source": [
    "print(prediction_val[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0. 15.  9. 24.  4. 18.  7.  4. 10. 15.]\n"
     ]
    }
   ],
   "source": [
    "print(Y_test[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
